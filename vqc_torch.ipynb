{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# VARIATIONAL QUANTUM CIRCUIT WITH PYTORCH\n",
    "# source: pennylane demos, kernel based training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane.templates import AngleEmbedding, StronglyEntanglingLayers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN DATA\n",
    "X_train = np.loadtxt(\"trainX.txt\") # size 1600\n",
    "y_train = np.loadtxt(\"trainY.txt\")\n",
    "X_test = np.loadtxt(\"testX.txt\") # size 256\n",
    "y_test = np.loadtxt(\"testY.txt\")\n",
    "\n",
    "# scaling the inputs is important since the embedding we use is periodic\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "scaler = StandardScaler().fit(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# swaps 0 with -1 so we classify by -1 and 1 instead of 0 and 1\n",
    "y_train = np.where(y_train == 0, -1.0, 1.0) \n",
    "y_test = np.where(y_test == 0, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the variational principle of training, we can propose an *ansatz*\n",
    "for the variational circuit and train it directly. By increasing the\n",
    "number of layers of the ansatz, its expressivity increases. Depending on\n",
    "the ansatz, we may only search through a subspace of all measurements\n",
    "for the best candidate.\n",
    "\n",
    "Remember from above, the variational training does not optimize\n",
    "*exactly* the same cost as the SVM, but we try to match them as closely\n",
    "as possible. For this we use a bias term in the quantum model, and train\n",
    "on the hinge loss.\n",
    "\n",
    "We also explicitly use the\n",
    "[parameter-shift](https://pennylane.ai/qml/glossary/parameter_shift.html)\n",
    "differentiation method in the quantum node, since this is a method which\n",
    "works on hardware as well. While `diff_method='backprop'` or\n",
    "`diff_method='adjoint'` would reduce the number of circuit evaluations\n",
    "significantly, they are based on tricks that are only suitable for\n",
    "simulators, and can therefore not scale to more than a few dozen qubits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_qubits = 2\n",
    "dev_var = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev_var, diff_method=\"parameter-shift\")\n",
    "def quantum_model(x, params):\n",
    "    \"\"\"A variational quantum model.\"\"\"\n",
    "\n",
    "    # embedding\n",
    "    AngleEmbedding(x, wires=range(n_qubits))\n",
    "\n",
    "    # trainable measurement\n",
    "    StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "def quantum_model_plus_bias(x, params, bias):\n",
    "    \"\"\"Adding a bias.\"\"\"\n",
    "    return quantum_model(x, params) + bias\n",
    "\n",
    "def hinge_loss(predictions, targets):\n",
    "    \"\"\"Implements the hinge loss.\"\"\"\n",
    "    all_ones = torch.ones_like(targets)\n",
    "    hinge_loss = all_ones - predictions * targets\n",
    "    # trick: since the max(0,x) function is not differentiable,\n",
    "    # use the mathematically equivalent relu instead\n",
    "    hinge_loss = relu(hinge_loss)\n",
    "    return hinge_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now summarize the usual training and prediction steps into two\n",
    "functions similar to scikit-learn\\'s `fit()` and `predict()`. While it\n",
    "feels cumbersome compared to the one-liner used to train the kernel\n",
    "method, PennyLane---like other differentiable programming\n",
    "libraries---provides a lot more control over the particulars of\n",
    "training.\n",
    "\n",
    "In our case, most of the work is to convert between numpy and torch,\n",
    "which we need for the differentiable `relu` function used in the hinge\n",
    "loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def quantum_model_train(n_layers, steps, batch_size):\n",
    "    \"\"\"Train the quantum model defined above.\"\"\"\n",
    "\n",
    "    params = np.random.random((n_layers, n_qubits, 3))\n",
    "    params_torch = torch.tensor(params, requires_grad=True)\n",
    "    bias_torch = torch.tensor(0.0)\n",
    "\n",
    "    opt = torch.optim.Adam([params_torch, bias_torch], lr=0.1)\n",
    "\n",
    "    loss_history = []\n",
    "    for i in range(steps):\n",
    "\n",
    "        batch_ids = np.random.choice(len(X_train), batch_size)\n",
    "\n",
    "        X_batch = X_train[batch_ids]\n",
    "        y_batch = y_train[batch_ids]\n",
    "\n",
    "        X_batch_torch = torch.tensor(X_batch, requires_grad=False)\n",
    "        y_batch_torch = torch.tensor(y_batch, requires_grad=False)\n",
    "\n",
    "        def closure():\n",
    "            opt.zero_grad()\n",
    "            preds = torch.stack(\n",
    "                [quantum_model_plus_bias(x, params_torch, bias_torch) for x in X_batch_torch]\n",
    "            )\n",
    "            loss = torch.mean(hinge_loss(preds, y_batch_torch))\n",
    "\n",
    "            # bookkeeping\n",
    "            current_loss = loss.detach().numpy().item()\n",
    "            loss_history.append(current_loss)\n",
    "            if i % 100 == 0:\n",
    "                print(\"step\", i, \", loss\", current_loss)\n",
    "\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        opt.step(closure)\n",
    "\n",
    "    return params_torch, bias_torch, loss_history\n",
    "\n",
    "\n",
    "def quantum_model_predict(X_pred, trained_params, trained_bias):\n",
    "    \"\"\"Predict using the quantum model defined above.\"\"\"\n",
    "\n",
    "    p = []\n",
    "    for x in X_pred:\n",
    "\n",
    "        x_torch = torch.tensor(x)\n",
    "        pred_torch = quantum_model_plus_bias(x_torch, trained_params, trained_bias)\n",
    "        pred = pred_torch.detach().numpy().item()\n",
    "        if pred > 0:\n",
    "            pred = 1\n",
    "        else:\n",
    "            pred = -1\n",
    "\n",
    "        p.append(pred)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the variational model and see how well we are doing on the\n",
    "test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dev_var\u001b[38;5;241m.\u001b[39mtracker:\n\u001b[0;32m----> 8\u001b[0m     trained_params, trained_bias, loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mquantum_model_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     pred_train \u001b[38;5;241m=\u001b[39m quantum_model_predict(X_train, trained_params, trained_bias)\n\u001b[1;32m     10\u001b[0m     pred_test \u001b[38;5;241m=\u001b[39m quantum_model_predict(X_test, trained_params, trained_bias)\n",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m, in \u001b[0;36mquantum_model_train\u001b[0;34m(n_layers, steps, batch_size)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantum_model_train\u001b[39m(n_layers, steps, batch_size):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the quantum model defined above.\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom((n_layers, n_qubits, \u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "n_layers = 6\n",
    "batch_size = 128\n",
    "steps = 10000\n",
    "\n",
    "with dev_var.tracker:\n",
    "    trained_params, trained_bias, loss_history = quantum_model_train(n_layers, steps, batch_size)\n",
    "    pred_train = quantum_model_predict(X_train, trained_params, trained_bias)\n",
    "    pred_test = quantum_model_predict(X_test, trained_params, trained_bias)\n",
    "\n",
    "print(\"accuracy on train set:\", accuracy_score(pred_train, y_train))\n",
    "print(\"accuracy on test set:\", accuracy_score(pred_test, y_test))\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print(\"runtime in minutes: \", (end-start)/60)\n",
    "\n",
    "# 100 steps: \n",
    "# accuracy on train set: 0.68625\n",
    "# accuracy on test set: 0.6796875\n",
    "\n",
    "# 10,000 steps: \n",
    "# accuracy on train set: 0.68625\n",
    "# accuracy on test set: 0.6796875\n",
    "\n",
    "# 10,000 steps, full data:\n",
    "# accuracy on train set: 0.71\n",
    "# accuracy on test set: 0.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational circuit has a slightly lower accuracy than the SVM---but\n",
    "this depends very much on the training settings we used. Different\n",
    "random parameter initializations, more layers, or more steps may indeed\n",
    "get perfect test accuracy.\n",
    "\n",
    "How often was the device executed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'executions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdev_var\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotals\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexecutions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'executions'"
     ]
    }
   ],
   "source": [
    "dev_var.tracker.totals['executions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a lot more than the kernel method took!\n",
    "\n",
    "Let's try to understand this value. In each optimization step, the\n",
    "variational circuit needs to compute the partial derivative of all\n",
    "trainable parameters for each sample in a batch. Using parameter-shift\n",
    "rules, we require roughly two circuit evaluations per partial\n",
    "derivative. Prediction uses only one circuit evaluation per sample.\n",
    "\n",
    "We can formulate this as another function that will be used in the\n",
    "scaling plot below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def circuit_evals_variational(n_data, n_params, n_steps, shift_terms, split, batch_size):\n",
    "    \"\"\"Compute how many circuit evaluations are needed for\n",
    "       variational training and prediction.\"\"\"\n",
    "\n",
    "    M = int(np.ceil(split * n_data))\n",
    "    Mpred = n_data - M\n",
    "\n",
    "    n_training = n_params * n_steps * batch_size * shift_terms\n",
    "    n_prediction = Mpred\n",
    "\n",
    "    return n_training + n_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This estimates the circuit evaluations in variational training as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m circuit_evals_variational(\n\u001b[0;32m----> 2\u001b[0m     n_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mX\u001b[49m),\n\u001b[1;32m      3\u001b[0m     n_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(trained_params\u001b[38;5;241m.\u001b[39mflatten()),\n\u001b[1;32m      4\u001b[0m     n_steps\u001b[38;5;241m=\u001b[39msteps,\n\u001b[1;32m      5\u001b[0m     shift_terms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      6\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(X_train) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(X_train) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_test)),\n\u001b[1;32m      7\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "circuit_evals_variational(\n",
    "    n_data=len(X),\n",
    "    n_params=len(trained_params.flatten()),\n",
    "    n_steps=steps,\n",
    "    shift_terms=2,\n",
    "    split=len(X_train) / (len(X_train) + len(X_test)),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimate is a bit higher because it does not account for some\n",
    "optimizations that PennyLane performs under the hood.\n",
    "\n",
    "It is important to note that while they are trained in a similar manner,\n",
    "the number of variational circuit evaluations differs from the number of\n",
    "neural network model evaluations in classical machine learning, which\n",
    "would be given by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def model_evals_nn(n_data, n_params, n_steps, split, batch_size):\n",
    "    \"\"\"Compute how many model evaluations are needed for neural\n",
    "       network training and prediction.\"\"\"\n",
    "\n",
    "    M = int(np.ceil(split * n_data))\n",
    "    Mpred = n_data - M\n",
    "\n",
    "    n_training = n_steps * batch_size\n",
    "    n_prediction = Mpred\n",
    "\n",
    "    return n_training + n_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each step of neural network training, and due to the clever\n",
    "implementations of automatic differentiation, the backpropagation\n",
    "algorithm can compute a gradient for all parameters in (more-or-less) a\n",
    "single run. For all we know at this stage, the no-cloning principle\n",
    "prevents variational circuits from using these tricks, which leads to\n",
    "`n_training` in `circuit_evals_variational` depending on the number of\n",
    "parameters, but not in `model_evals_nn`.\n",
    "\n",
    "For the same example as used here, a neural network would therefore have\n",
    "far fewer model evaluations than both variational and kernel-based\n",
    "training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_evals_nn(\n",
    "    n_data=len(X),\n",
    "    n_params=len(trained_params.flatten()),\n",
    "    n_steps=steps,\n",
    "    split=len(X_train) / (len(X_train) + len(X_test)),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which method scales best?\n",
    "=========================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer to this question depends on how the variational model is set\n",
    "up, and we need to make a few assumptions:\n",
    "\n",
    "1.  Even if we use single-batch stochastic gradient descent, in which\n",
    "    every training step uses exactly one training sample, we would want\n",
    "    to see every training sample at least once on average. Therefore,\n",
    "    the number of steps should scale at least linearly with the number\n",
    "    of training data samples.\n",
    "\n",
    "2.  Modern neural networks often have many more parameters than training\n",
    "    samples. But we do not know yet whether variational circuits really\n",
    "    need that many parameters as well. We will therefore use two cases\n",
    "    for comparison:\n",
    "\n",
    "    2a) the number of parameters grows linearly with the training data,\n",
    "    or `n_params = M`,\n",
    "\n",
    "    2b) the number of parameters saturates at some point, which we model\n",
    "    by setting `n_params = sqrt(M)`.\n",
    "\n",
    "Note that compared to the example above with 75 training samples and 24\n",
    "parameters, a) overestimates the number of evaluations, while b)\n",
    "underestimates it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the three methods compare:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "variational_training1 = []\n",
    "variational_training2 = []\n",
    "kernelbased_training = []\n",
    "nn_training = []\n",
    "x_axis = range(0, 2000, 100)\n",
    "\n",
    "for M in x_axis:\n",
    "    var1 = circuit_evals_variational(\n",
    "        n_data=M, n_params=M, n_steps=M, shift_terms=2, split=0.75, batch_size=1\n",
    "    )\n",
    "    variational_training1.append(var1)\n",
    "\n",
    "    var2 = circuit_evals_variational(\n",
    "        n_data=M, n_params=round(np.sqrt(M)), n_steps=M,\n",
    "        shift_terms=2, split=0.75, batch_size=1\n",
    "    )\n",
    "    variational_training2.append(var2)\n",
    "\n",
    "    kernel = circuit_evals_kernel(n_data=M, split=0.75)\n",
    "    kernelbased_training.append(kernel)\n",
    "\n",
    "    nn = model_evals_nn(\n",
    "        n_data=M, n_params=M, n_steps=M, split=0.75, batch_size=1\n",
    "    )\n",
    "    nn_training.append(nn)\n",
    "\n",
    "\n",
    "plt.plot(x_axis, nn_training, linestyle='--', label=\"neural net\")\n",
    "plt.plot(x_axis, variational_training1, label=\"var. circuit (linear param scaling)\")\n",
    "plt.plot(x_axis, variational_training2, label=\"var. circuit (srqt param scaling)\")\n",
    "plt.plot(x_axis, kernelbased_training, label=\"(quantum) kernel\")\n",
    "plt.xlabel(\"size of data set\")\n",
    "plt.ylabel(\"number of evaluations\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the plot we saw at the beginning. With current\n",
    "hardware-compatible training methods, whether kernel-based training\n",
    "requires more or fewer quantum circuit evaluations than variational\n",
    "training depends on how many parameters the latter needs. If variational\n",
    "circuits turn out to be as parameter-hungry as neural networks,\n",
    "kernel-based training will outperform them for common machine learning\n",
    "tasks. However, if variational learning only turns out to require few\n",
    "parameters (or if more efficient training methods are found),\n",
    "variational circuits could in principle match the linear scaling of\n",
    "neural networks trained with backpropagation.\n",
    "\n",
    "The practical take-away from this demo is that unless your variational\n",
    "circuit has significantly fewer parameters than training data, kernel\n",
    "methods could be a much faster alternative!\n",
    "\n",
    "Finally, it is important to note that fault-tolerant quantum computers\n",
    "may change the picture for both quantum and classical machine learning.\n",
    "As mentioned in [Schuld (2021)](https://arxiv.org/abs/2101.11020), early\n",
    "results from the quantum machine learning literature show that larger\n",
    "quantum computers will most likely enable us to reduce the quadratic\n",
    "scaling of kernel methods to linear scaling, which may make classical as\n",
    "well as quantum kernel methods a strong alternative to neural networks\n",
    "for big data processing one day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the author\n",
    "================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
